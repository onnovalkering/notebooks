{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMPCamp 6\n",
    "http://ampcamp.berkeley.edu/6\n",
    "\n",
    "### Preparation\n",
    "This assumes that you're running the Bitnami Hadoop VM version 3.3.0 (October 2020).\n",
    "\n",
    "Before we can start with the AMPCamp 6 tutorial, we've to prepare our VM for PySpark.\n",
    "\n",
    "First, we need to install some additional dependencies:\n",
    "```shell\n",
    "sudo apt install python3-requests python3-notebook jupyter jupyter-core \n",
    "```\n",
    "\n",
    "Also, we have to apply the following configuration:\n",
    "```shell\n",
    "export PYSPARK_PYTHON=python3\n",
    "export PATH=\"$PATH:$HOME/stack/hadoop/spark/bin\"\n",
    "\n",
    "sudo ufw allow 8888/tcp\n",
    "sudo ufw allow 4040/tcp\n",
    "```\n",
    "\n",
    "Now, to start a PySpark shell session:\n",
    "```shell\n",
    "pyspark\n",
    "```\n",
    "\n",
    "Or, to start [Jupyter](https://jupyter.org), with support for PySpark (copy the URL that will appear in a browser on your host machine):\n",
    "```shell\n",
    "PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --ip=$(hostname -I)\" pyspark\n",
    "```\n",
    "\n",
    "We continue by downloading the data used in the tutorial. Run the code from the following cells into your PySpark shell or notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import itertools\n",
    "import requests\n",
    "import os\n",
    "import pyspark\n",
    "import shutil\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./data/pagecounts\n",
    "\n",
    "base_url = 'https://archive.org/download/wikipedia_visitor_stats_200905'\n",
    "filenames = ['pagecounts-20090505-000000.gz', 'pagecounts-20090507-000000.gz']\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "for filename in filenames:\n",
    "    with requests.get(f'{base_url}/{filename}', stream=True) as stream:\n",
    "        with open(filename, 'wb') as file:\n",
    "            shutil.copyfileobj(stream.raw, file)\n",
    "        #\n",
    "        filename_noext = os.path.splitext(filename)[0]\n",
    "        with gzip.open(filename, 'rb') as src, open(filename_noext, 'wb') as dest:\n",
    "            for chunk in iter(lambda : src.read(100 * 1024), b''):\n",
    "                dest.write(chunk)\n",
    "        #\n",
    "        datetime = filename_noext[11:].encode('UTF-8')\n",
    "        with open(filename_noext, 'rb') as src, open('data/pagecounts', 'ab') as dest:\n",
    "            for line in src:\n",
    "                dest.write(datetime + b' ' + line)\n",
    "        #\n",
    "        os.remove(filename)\n",
    "        os.remove(filename_noext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./data/wiki.parquet\n",
    "\n",
    "base_url = 'https://github.com/databricks/spark-training/raw/master/data/wiki_parquet/'\n",
    "filenames = [\n",
    "    '_SUCCESS', '._metadata', 'part-r-1.parquet', \n",
    "    'part-r-2.parquet', 'part-r-3.parquet', 'part-r-4.parquet',\n",
    "    'part-r-5.parquet', 'part-r-6.parquet', 'part-r-7.parquet', \n",
    "    'part-r-8.parquet', 'part-r-9.parquet', 'part-r-10.parquet'        \n",
    "]\n",
    "\n",
    "os.makedirs('data/wiki.parquet', exist_ok=True)\n",
    "\n",
    "for filename in filenames:\n",
    "    with requests.get(f'{base_url}/{filename}', stream=True) as stream:\n",
    "        with open(f'data/wiki.parquet/{filename}', 'wb') as file:\n",
    "            shutil.copyfileobj(stream.raw, file)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step, make sure the `sc` and `sqlContext` PySpark objects exists in your PySpark shell or notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your environment is ready to follow the AMPCamp 6 tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration Using Spark\n",
    "http://ampcamp.berkeley.edu/6/exercises/data-exploration-using-spark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e7550b514205:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/pagecounts MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts = sc.textFile(\"data/pagecounts\")\n",
    "pagecounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20090505-000000 aa.b ?71G4Bo1cAdWyg 1 14463\n",
      "20090505-000000 aa.b Special:Statistics 1 840\n",
      "20090505-000000 aa.b Special:Whatlinkshere/MediaWiki:Returnto 1 1019\n",
      "20090505-000000 aa.b Wikibooks:About 1 15719\n",
      "20090505-000000 aa ?14mFX1ildVnBc 1 13205\n",
      "20090505-000000 aa ?53A%2FuYP3FfnKM 1 13207\n",
      "20090505-000000 aa ?93HqrnFc%2EiqRU 1 13199\n",
      "20090505-000000 aa ?95iZ%2Fjuimv31g 1 13201\n",
      "20090505-000000 aa File:Wikinews-logo.svg 1 8357\n",
      "20090505-000000 aa Main_Page 2 9980\n"
     ]
    }
   ],
   "source": [
    "for x in pagecounts.take(10):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8020573"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3537328"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enPages = pagecounts.filter(lambda x: x.split(\" \")[1] == \"en\").cache()\n",
    "enPages.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "enTuples = enPages.map(lambda x: x.split(\" \"))\n",
    "enKeyValuePairs = enTuples.map(lambda x: (x[0][:8], int(x[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('20090505', 8822061), ('20090507', 8751659)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enKeyValuePairs.reduceByKey(lambda x, y: x + y, 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('20090505', 8822061), ('20090507', 8751659)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enPages.map(lambda x: x.split(\" \")).map(lambda x: (x[0][:8], int(x[3]))).reduceByKey(lambda x, y: x + y, 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(479251, 'Special:Search'), (1133206, '404_error/'), (482021, 'Main_Page')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced = enPages.map(lambda x: x.split(\" \")).map(lambda x: (x[2], int(x[3]))).reduceByKey(lambda x, y: x + y, 40)\n",
    "filtered = reduced.filter(lambda x: x[1] > 200000).map(lambda x: (x[1], x[0]))\n",
    "\n",
    "filtered.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration Using Spark SQL\n",
    "http://ampcamp.berkeley.edu/6/exercises/data-exploration-using-spark-sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e7550b514205:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc8b79d07b8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiData = sqlContext.read.parquet(\"data/wiki.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39365"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiData.registerTempTable(\"wikiData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sqlContext.sql(\"SELECT COUNT(*) AS pageCount FROM wikiData\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39365"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].pageCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(username='Waacstats', cnt=2003),\n",
       " Row(username='Cydebot', cnt=949),\n",
       " Row(username='BattyBot', cnt=939),\n",
       " Row(username='Yobot', cnt=890),\n",
       " Row(username='Addbot', cnt=853),\n",
       " Row(username='Monkbot', cnt=668),\n",
       " Row(username='ChrisGualtieri', cnt=438),\n",
       " Row(username='RjwilmsiBot', cnt=387),\n",
       " Row(username='OccultZone', cnt=377),\n",
       " Row(username='ClueBot NG', cnt=353)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT username, COUNT(*) AS cnt FROM wikiData WHERE username <> '' GROUP BY username ORDER BY cnt DESC LIMIT 10\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
